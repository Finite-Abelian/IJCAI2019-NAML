{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "with open('ClickData4.tsv') as f:\n",
    "    user=f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('ClickData4.tsv','w') as f:\n",
    "    for i in user:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "def timeconvert(timestr):\n",
    "    ifpm=False\n",
    "    if 'PM' in timestr:\n",
    "        ifpm=True\n",
    "    tp=datetime.datetime.strptime(timestr[:-3], \"%m/%d/%Y %H:%M:%S\")\n",
    "    tp+=datetime.timedelta(hours=12)\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('DocMeta3.tsv') as f:\n",
    "    data=f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "news={}\n",
    "category={}\n",
    "subcategory={}\n",
    "for i in data:\n",
    "    tp=i.strip().split('\\t')\n",
    "    news[tp[1]]=[tp[2],tp[3],word_tokenize(tp[6].lower()),word_tokenize(tp[7].lower())]\n",
    "    category[tp[2]]=0\n",
    "    subcategory[tp[3]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category={'None':0}\n",
    "subcategory={'None':0}\n",
    "for i in data:\n",
    "    tp=i.strip().split('\\t')\n",
    "    if tp[2] not in category:\n",
    "        category[tp[2]]=len(category)\n",
    "    if tp[3] not in subcategory:\n",
    "        subcategory[tp[3]]=len(subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict={}\n",
    "for i in user:\n",
    "    tr=i.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if userid not in  userid_dict:\n",
    "        userid_dict[userid]=len(userid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict0={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict0:\n",
    "            word_dict0[j][1]+=1\n",
    "        else:\n",
    "            word_dict0[j]=[len(word_dict0),1]\n",
    "for i in news:\n",
    "    for j in news[i][3]:\n",
    "        if j in word_dict0:\n",
    "            word_dict0[j][1]+=1\n",
    "        else:\n",
    "            word_dict0[j]=[len(word_dict0),1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for i in word_dict0:\n",
    "    if word_dict0[i][1]>=3:\n",
    "        word_dict[i]=[len(word_dict),word_dict0[i][1]]\n",
    "print(len(word_dict),len(word_dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embdict={}\n",
    "plo=0\n",
    "import pickle\n",
    "with open('/data/wuch/glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp\n",
    "                if plo%100==0:\n",
    "                    print(plo,linenb,word)\n",
    "                plo+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky\n",
    "word_dict1=word_dict\n",
    "print(len(embdict),len(word_dict1))\n",
    "print(len(word_dict1))\n",
    "lister=[0]*len(word_dict1)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict1[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict1[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words=[[0]*30]\n",
    "news_index={'0':0}\n",
    "for i in news:\n",
    "    tp=[]\n",
    "    news_index[i]=len(news_index)\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            tp.append(word_dict[j][0])\n",
    "    tp=tp[:30]\n",
    "    news_words.append(tp+[0]*(30-len(tp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_body=[[0]*300]\n",
    "news_index={'0':0}\n",
    "for i in news:\n",
    "    tp=[]\n",
    "    news_index[i]=len(news_index)\n",
    "    for j in news[i][3]:\n",
    "        if j in word_dict:\n",
    "            tp.append(word_dict[j][0])\n",
    "    tp=tp[:300]\n",
    "    news_body.append(tp+[0]*(300-len(tp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_v=[to_categorical(0,len(category))]\n",
    "news_sv=[to_categorical(0,len(subcategory))]\n",
    "for i in news:\n",
    "    news_v.append(to_categorical(category[news[i][0]],len(category)))\n",
    "for i in news:\n",
    "    news_sv.append(to_categorical(subcategory[news[i][1]],len(subcategory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_body=np.array(news_body,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_v=np.array(news_v,dtype='int32') \n",
    "news_sv=np.array(news_sv,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "npratio=4\n",
    "all_train_id=[]\n",
    "all_train_pn=[]    \n",
    "all_labeler=[]\n",
    "\n",
    "all_test_id=[]\n",
    "all_test_pn=[]    \n",
    "all_test_labeler=[]\n",
    "all_test_index=[]\n",
    "\n",
    "all_user_pos=[]\n",
    "all_test_user_pos=[]\n",
    "\n",
    "for raw in user:\n",
    "    tr=raw.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if len(tr)==4:\n",
    "        \n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "    if len(tr)==3:\n",
    "        tp=[x.split('#TAB#') for x in tr[2].split('#N#')]\n",
    "\n",
    "    trainpos=[x[0].split() for x in tp]\n",
    "    trainneg=[x[1].split() for x in tp]\n",
    "     \n",
    "    ppp=list(itertools.chain(*(trainpos)))\n",
    "    nnn=list(itertools.chain(*(trainneg)))\n",
    "    #print(ppp)\n",
    "    \n",
    "    \n",
    "    if len(tr)==4:\n",
    "        tp=[x.split('#TAB#') for x in tr[3].split('#N#')]\n",
    "        testpos=[x[0].split() for x in tp]\n",
    "        testneg=[x[1].split() for x in tp]\n",
    "        \n",
    "        \n",
    "        for i in range(len(testpos)):\n",
    "            tp=[]\n",
    "            tp.append(len(all_test_pn))\n",
    "            qqq=list(set(ppp))\n",
    "            allpos=[news_index[p] for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "    \n",
    "            \n",
    "            for j in testpos[i]:\n",
    "                all_test_pn.append(news_index[j])\n",
    "                all_test_labeler.append(1)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "                \n",
    "            for j in testneg[i]:\n",
    "                all_test_pn.append(news_index[j])\n",
    "                all_test_labeler.append(0)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "            tp.append(len(all_test_pn))\n",
    "            all_test_index.append(tp)\n",
    "            \n",
    "    #nnnnn=list(set(nnn)-set(ppp))\n",
    "\n",
    "            \n",
    "    for mp in range(len(trainpos)):\n",
    "        for ps in trainpos[mp]:\n",
    "            #tql=list(set(nnn)-set(trainpos[mp]))\n",
    "            negps=newsample(trainneg[mp],npratio)\n",
    "            negps.append(ps)\n",
    "            tplb=[0]*npratio+[1]\n",
    "            tid=list(range(npratio+1))\n",
    "            random.shuffle(tid)\n",
    "\n",
    "            \n",
    "            yp=[]\n",
    "            yplb=[]\n",
    "            for j in tid:\n",
    "                yp.append(news_index[negps[j]])\n",
    "                yplb.append(tplb[j])\n",
    "            \n",
    "            qqq=list(set(ppp)-set([ps]))\n",
    "            allpos=[news_index[p] for p in random.sample(qqq,min(50,len(qqq)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "            all_train_pn.append(yp)\n",
    "            all_labeler.append(yplb)\n",
    "            all_train_id.append(userid_dict[userid])\n",
    "            all_user_pos.append(allpos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random2(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    np.random.shuffle(idx)\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            itx2 = news_body[all_train_pn[i]]\n",
    "            itx3 = news_v[all_train_pn[i]]\n",
    "            itx4 = news_sv[all_train_pn[i]]\n",
    "            tkk=all_user_pos[all_train_id[i]]\n",
    "            itxx=[itx[:,k,:] for k in range(itx.shape[1])]\n",
    "            itxx2=[itx2[:,k,:] for k in range(itx2.shape[1])]\n",
    "            itxx3=[itx3[:,k,:] for k in range(itx3.shape[1])]\n",
    "            itxx4=[itx4[:,k,:] for k in range(itx4.shape[1])]\n",
    "            usx=news_words[all_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            usx2=news_body[all_user_pos[i]]\n",
    "            usxx2=[usx2[:,k,:] for k in range(usx2.shape[1])]\n",
    "            usx3=news_v[all_user_pos[i]]\n",
    "            usxx3=[usx3[:,k,:] for k in range(usx3.shape[1])]\n",
    "            usx4=news_sv[all_user_pos[i]]\n",
    "            usxx4=[usx4[:,k,:] for k in range(usx4.shape[1])]\n",
    "            yy=all_labeler[i]\n",
    "            \n",
    "            yield (itxx +usxx+itxx2 +usxx2+itxx3 +usxx3+itxx4 +usxx4, [yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data2(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            itx2 = news_body[all_train_pn[i]]\n",
    "            itx3 = news_v[all_train_pn[i]]\n",
    "            itx4 = news_sv[all_train_pn[i]]\n",
    "            usx=news_words[all_test_user_pos[i]]\n",
    "            usxx=[usx[:,k,:] for k in range(usx.shape[1])]\n",
    "            usx2=news_body[all_test_user_pos[i]]\n",
    "            usxx2=[usx2[:,k,:] for k in range(usx2.shape[1])]\n",
    "            usx3=news_v[all_test_user_pos[i]]\n",
    "            usxx3=[usx3[:,k,:] for k in range(usx3.shape[1])]\n",
    "            usx4=news_sv[all_test_user_pos[i]]\n",
    "            usxx4=[usx4[:,k,:] for k in range(usx3.shape[1])]\n",
    "            yy=all_labeler[i]\n",
    "            yield ([itx]+ usxx+[itx2]+ usxx2+[itx3]+ usxx3+[itx4]+ usxx4, [yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_pn=np.array(all_train_pn,dtype='int32')\n",
    "all_labeler=np.array(all_labeler,dtype='int32')\n",
    "all_train_id=np.array(all_train_id,dtype='int32')\n",
    "all_test_pn=np.array(all_test_pn,dtype='int32')\n",
    "all_test_labeler=np.array(all_test_labeler,dtype='int32')\n",
    "all_test_id=np.array(all_test_id,dtype='int32')\n",
    "all_user_pos=np.array(all_user_pos,dtype='int32')\n",
    "all_test_user_pos=np.array(all_test_user_pos,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import keras\n",
    "import random\n",
    "results=[]\n",
    "keras.backend.clear_session()\n",
    "for npratio in range(4,5):\n",
    "\n",
    "    MAX_SENT_LENGTH=30\n",
    "    MAX_SENTS=50\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    MAX_SENT_LENGTH2=300\n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    \n",
    "    sentence_input2 = Input(shape=(MAX_SENT_LENGTH2,), dtype='int32')\n",
    "    embedding_layer = Embedding(len(word_dict), 300, weights=[lister],trainable=True)\n",
    "    \n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    d_et=Dropout(0.2)(embedded_sequences)\n",
    "    \n",
    "    embedded_sequences2 = embedding_layer(sentence_input2)\n",
    "    d_et2=Dropout(0.2)(embedded_sequences2)\n",
    "    \n",
    "    l_cnnt = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_et)\n",
    "    d_ct=Dropout(0.2)(l_cnnt)\n",
    "    \n",
    "    attention = Dense(200,activation='tanh')(d_ct)\n",
    "    attention = Flatten()(Dense(1)(attention))\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    l_attt=keras.layers.Dot((1, 1))([d_ct, attention_weight])\n",
    "    \n",
    "    l_cnnt2 = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_et2)\n",
    "    d_ct2=Dropout(0.2)(l_cnnt2)\n",
    "    \n",
    "    attention2 = Dense(200,activation='tanh')(d_ct2)\n",
    "    attention2 = Flatten()(Dense(1)(attention2))\n",
    "    attention_weight2 = Activation('softmax')(attention2)\n",
    "    l_attt2=keras.layers.Dot((1, 1))([d_ct2, attention_weight2])\n",
    "\n",
    "    vinput=Input((1,), dtype='int32') \n",
    "    svinput=Input((1,), dtype='int32') \n",
    "    v_embedding_layer = Embedding(len(category)+1, 50,trainable=True)\n",
    "    sv_embedding_layer = Embedding(len(subcategory)+1, 50,trainable=True)\n",
    "    v_embedding=Dense(400,activation='relu')(Flatten()(v_embedding_layer(vinput)))\n",
    "    sv_embedding=Dense(400,activation='relu')(Flatten()(sv_embedding_layer(svinput)))\n",
    "\n",
    "    all_channel=[l_attt,l_attt2,v_embedding,sv_embedding]\n",
    "        \n",
    "    candidate_vecssent50=concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(channel) for channel in all_channel],axis=1)\n",
    "    \n",
    "    attentionv = Dense(200,activation='tanh')(candidate_vecssent50)\n",
    "    \n",
    "    attention_weightv =Lambda(lambda x:K.squeeze(x,axis=-1))(Dense(1)(attentionv))\n",
    "    attention_weightv =Activation('softmax')(attention_weightv)\n",
    "\n",
    "    weightv=keras.layers.Dot((1, 1))([candidate_vecssent50, attention_weightv])\n",
    "    \n",
    "    sentEncodert = Model([sentence_input,sentence_input2,vinput,svinput],weightv)\n",
    "    \n",
    "    review_input = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    \n",
    "    review_input2 = [keras.Input((MAX_SENT_LENGTH2,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    \n",
    "    review_input3 = [keras.Input((1,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    \n",
    "    review_input4 = [keras.Input((1,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "    candidate_4vecssent = [sentEncodert([review_input[_],review_input2[_],review_input3[_],review_input4[_] ]) for _ in range(MAX_SENTS)]\n",
    "    candidate_vecssent5 =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(review) for review in candidate_4vecssent],axis=1)    \n",
    "    \n",
    "    attentionn = Dense(200,activation='tanh')(candidate_vecssent5)\n",
    "    attentionn =Flatten()(Dense(1)(attentionn))\n",
    "    attention_weightn = Activation('softmax')(attentionn)\n",
    "    l_att2=keras.layers.Dot((1, 1))([candidate_vecssent5, attention_weightn])\n",
    "    #l_att2=AttLayer()(candidate_vecssent2)\n",
    "    \n",
    "    candidates = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "    \n",
    "    candidates2 = [keras.Input((MAX_SENT_LENGTH2,), dtype='int32') for _ in range(1+npratio)]\n",
    "    \n",
    "    candidates3 = [keras.Input((1,), dtype='int32') for _ in range(1+npratio)]\n",
    "    \n",
    "    candidates4 = [keras.Input((1,), dtype='int32') for _ in range(1+npratio)]\n",
    "    candidate_vecs5 = [sentEncodert([candidates[_],user_id,candidates2[_],candidates3[_],candidates4[_]]) for _ in range(1+npratio)]\n",
    "    \n",
    "    logits = [keras.layers.dot([l_att2, candidate_vec], axes=-1) for candidate_vec in candidate_vecs5]\n",
    "    logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "\n",
    "    \n",
    "    model = Model(candidates+review_input+candidates2+review_input2+candidates3+review_input3+candidates4+review_input4, logits)\n",
    "    \n",
    "    \n",
    "    candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "    candidate_one_vec = sentEncodert([candidate_one])\n",
    "    \n",
    "    candidate_one2 = keras.Input((MAX_SENT_LENGTH2,))\n",
    "    candidate_one_vec2 = sentEncodert2([candidate_one2])\n",
    "    \n",
    "    candidate_one3 = keras.Input((1,))\n",
    "    candidate_one_vec3 = sentEncodert3([candidate_one3])\n",
    "    \n",
    "    candidate_one4 = keras.Input((1,))\n",
    "    candidate_one_vec4 = sentEncodert4([candidate_one4])\n",
    "    \n",
    "    candidate_one_vec5=sentEncodert([candidate_one_vec,candidate_one_vec2,candidate_one_vec3,candidate_one_vec4])\n",
    "    \n",
    "    score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "        keras.layers.dot([l_att2, candidate_one_vec5], axes=-1))\n",
    "    model2 = keras.Model([candidate_one,candidate_one2,candidate_one3,candidate_one4]\n",
    "                         +review_input+review_input2+review_input3+review_input4, score)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "    for ep in range(3):\n",
    "        traingen=generate_batch_data_random2(all_train_pn,all_labeler,all_train_id, 30)\n",
    "        \n",
    "        model.fit_generator(traingen, epochs=1,steps_per_epoch=len(all_train_id)//30)\n",
    "        #for ep in range(1):    \n",
    "        valgen=generate_batch_data2(all_test_pn,all_test_labeler,all_test_id, 30)\n",
    "        cr = model2.predict_generator(valgen, steps=len(all_test_id)//30,verbose=1)\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        all_auc=[]\n",
    "        all_mrr=[]\n",
    "        all_ndcg=[]\n",
    "        all_ndcg2=[]\n",
    "        for m in all_test_index:\n",
    "            if np.sum(all_test_labeler[m[0]:m[1]])!=0 and m[1]<len(cr):\n",
    "        \n",
    "                all_auc.append(roc_auc_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_mrr.append(mrr_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "                all_ndcg.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=5))\n",
    "                all_ndcg2.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=10))\n",
    "        results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "        print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
